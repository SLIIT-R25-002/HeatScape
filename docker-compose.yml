services:
  image-processing-service:
    build:
      context: ./image-processing-service
      dockerfile: Dockerfile
    ports:
      - "5001:5000"
    volumes:
      - ./image-processing-service:/app:ro
      - ./image-processing-service/uploads:/app/uploads
      - ./image-processing-service/weights:/app/weights
    runtime: nvidia  # Commented out due to WSL compatibility issues
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Commented out due to WSL compatibility issues
      - FLASK_ENV=development
      - FLASK_DEBUG=1
    command: ["python", "-m", "flask", "run", "--host=0.0.0.0", "--reload"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - heatscape-network

  vlm-solution-service:
    build:
      context: ./vlm-solution-service/backend
      dockerfile: Dockerfile
    ports:
      - "5002:5000"
    volumes:
      - ./vlm-solution-service/backend:/app:ro
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    command: ["python", "-m", "flask", "run", "--host=0.0.0.0", "--reload"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - heatscape-network

  iot-localization-service:
    build:
      context: ./iot-localization-service/backend
      dockerfile: Dockerfile
    ports:
      - "5004:5000"
    volumes:
      - ./iot-localization-service/backend:/app:ro
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - FLASK_ENV=development
      - FLASK_DEBUG=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - heatscape-network

  uhi-simulation-service:
    build:
      context: ./uhi-simulation-service/backend
      dockerfile: Dockerfile
    ports:
      - "4200:4200"
    environment:
      - NODE_ENV=development
      - PORT=4200
    volumes:
      # Mount source files individually to preserve node_modules
      - ./uhi-simulation-service/backend/server.js:/app/server.js:ro
      - ./uhi-simulation-service/backend/package.json:/app/package.json:ro
      - ./uhi-simulation-service/backend/.env:/app/.env:ro
      - ./uhi-simulation-service/backend/uploads:/app/uploads
      - ./uhi-simulation-service/backend/results:/app/results
    command: ["npm", "run", "start"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - heatscape-network
    depends_on:
      - vlm-solution-service

networks:
  heatscape-network:
    driver: bridge

volumes:
  image-uploads:
    driver: local
